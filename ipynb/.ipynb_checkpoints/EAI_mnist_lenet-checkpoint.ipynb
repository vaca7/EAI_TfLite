{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49ab3b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mnist = mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Normalize the pixels in 0.0~1.0 float\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1dc8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class mnist_lenet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(mnist_lenet, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(filters=10, kernel_size=[3,3], input_shape = (28,28,1), activation= 'relu')\n",
    "        self.pool1 = layers.MaxPooling2D(2, 2)\n",
    "        self.conv2 = layers.Conv2D(filters=20, kernel_size=[3,3], activation= 'relu')\n",
    "        self.pool2 = layers.MaxPooling2D(2, 2)\n",
    "        self.conv3 = layers.Conv2D(filters=30, kernel_size=[3,3], activation= 'relu')\n",
    "        self.flat = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(10, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        net = self.conv1(x)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.flat(net)\n",
    "        net = self.dense1(net)\n",
    "        net = self.dense2(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bef7a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 41s 679us/step - loss: 0.2440 - accuracy: 0.9228\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 42s 696us/step - loss: 0.0710 - accuracy: 0.9793\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 41s 680us/step - loss: 0.0675 - accuracy: 0.9814\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 59s 978us/step - loss: 0.0673 - accuracy: 0.9826\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 51s 854us/step - loss: 0.0678 - accuracy: 0.9826\n",
      "Model: \"mnist_lenet_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            multiple                  100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            multiple                  1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            multiple                  5430      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  17344     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  650       \n",
      "=================================================================\n",
      "Total params: 25,344\n",
      "Trainable params: 25,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: C:/Users/RTCV/EAI_TfLite/tmp\\assets\n"
     ]
    }
   ],
   "source": [
    "# Train & save model in frozen(.pb) format.\n",
    "\n",
    "my_model = mnist_lenet()\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "my_model.fit(train_images, train_labels, batch_size=1, epochs=5, verbose=1)\n",
    "my_model.summary()\n",
    "my_model.save('C:/Users/RTCV/EAI_TfLite/tmp') # saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0787c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and convert it for TensorFlow Lite (.tflite format)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'C:/Users/RTCV/EAI_TfLite/tmp'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0c14fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104484"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# Create folder to save model.\n",
    "tflite_models_dir = pathlib.Path(\"C:/Users/RTCV/EAI_TfLite/tmp/mnist_lenet/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_lenet.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ab3028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_input_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mint8  \u001b[38;5;66;03m# or tf.uint8\u001b[39;00m\n\u001b[0;32m     19\u001b[0m converter\u001b[38;5;241m.\u001b[39minference_output_type \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mint8  \u001b[38;5;66;03m# or tf.uint8\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RTCV\\anaconda3\\envs\\AIE\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:747\u001b[0m, in \u001b[0;36mTFLiteSavedModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    744\u001b[0m flags_modify_model_io_type \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mflags_modify_model_io_type(\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_input_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_output_type)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags_modify_model_io_type:\n\u001b[1;32m--> 747\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43m_modify_model_io_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflags_modify_model_io_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_sparsify_model:\n\u001b[0;32m    750\u001b[0m   result \u001b[38;5;241m=\u001b[39m _mlir_sparsify(result)\n",
      "File \u001b[1;32mc:\\Users\\RTCV\\anaconda3\\envs\\AIE\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:835\u001b[0m, in \u001b[0;36mmodify_model_io_type\u001b[1;34m(model, inference_input_type, inference_output_type)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_input_type \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    832\u001b[0m     inference_output_type \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    833\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 835\u001b[0m model_object \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_model_from_bytearray_to_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_object\u001b[38;5;241m.\u001b[39msubgraphs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    838\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel must only have one subgraph. Instead, it has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    839\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m subgraphs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(model_object\u001b[38;5;241m.\u001b[39msubgraphs)))\n",
      "File \u001b[1;32mc:\\Users\\RTCV\\anaconda3\\envs\\AIE\\lib\\site-packages\\tensorflow\\lite\\python\\util.py:572\u001b[0m, in \u001b[0;36m_convert_model_from_bytearray_to_object\u001b[1;34m(model_bytearray)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_model_from_bytearray_to_object\u001b[39m(model_bytearray):\n\u001b[0;32m    571\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a tflite model from a bytearray into a parsable object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m   model_object \u001b[38;5;241m=\u001b[39m \u001b[43mschema_fb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241m.\u001b[39mGetRootAsModel(model_bytearray, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    573\u001b[0m   model_object \u001b[38;5;241m=\u001b[39m schema_fb\u001b[38;5;241m.\u001b[39mModelT\u001b[38;5;241m.\u001b[39mInitFromObj(model_object)\n\u001b[0;32m    574\u001b[0m   model_object \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model_object)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'"
     ]
    }
   ],
   "source": [
    "# For Edge TPU.\n",
    "# You must quntize your model in INT8 precision.\n",
    "\n",
    "# Get representative data set for post-quantization.\n",
    "# The representative data set prevents accuracy drop while quantization.\n",
    "def representative_data_gen():\n",
    "    for image in train_images[:1000]:  # Use a subset of the dataset\n",
    "        # Resize the image to the input shape of your model\n",
    "        image = tf.image.resize(image, (28, 28))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        yield [image]\n",
    "\n",
    "model_path = 'C:/Users/RTCV/EAI_TfLite/tmp/'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Create folder to save model.\n",
    "tflite_models_dir = pathlib.Path(\"C:/Users/RTCV/EAI_TfLite/tmp/mnist_lenet/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_lenet_quant.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "\n",
    "# Now, you can convert your quzntized model for Edge TPU with edgetpu_compiler.\n",
    "# follow https://coral.ai/docs/edgetpu/compiler/#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9b137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
